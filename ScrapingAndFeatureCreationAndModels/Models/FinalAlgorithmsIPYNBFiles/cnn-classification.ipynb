{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport pandas as pd\nfrom sklearn import metrics\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv1D, Flatten,Activation\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.backend import sigmoid\nfrom sklearn.metrics import confusion_matrix\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\nfrom math import sqrt\nfrom multiprocessing.pool import ThreadPool\nimport time\nfrom sklearn.model_selection import GridSearchCV\nimport multiprocessing\nimport traceback\nimport tensorflow as tf\nfrom sklearn.metrics import precision_score, make_scorer\nfrom tensorflow.keras import backend as K\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pre_process_data(data,null_threshold):\n    \"\"\"\n    Drops Date and Unix Date columns from the data.\n    Drops the columns which has null values more than specified null_threshold.\n    Replaces infinite values with NAN.\n    Drops the rows which has null values.\n\n    Parameters\n    ----------\n    data : dataframe\n\n    null_threshold : numeric\n        numeric value describing the amount of null values that can be present.\n\n    Returns\n    -------\n    data : dataframe\n        an updated dataframe after performing all the opertaions.\n    \"\"\"\n    \n    data.drop(columns=['Unix Date','Date'],axis=1,inplace=True)\n    total = data.shape[0]\n    for col in data.columns:\n        if null_threshold * total / 100 < data[col].isnull().sum():\n            data.drop(columns=[col],axis=1,inplace=True)\n    data.replace([np.inf, -np.inf], np.nan, inplace=True)\n    data = data.apply(pd.to_numeric,errors='coerce')\n    data.dropna(axis=0,inplace=True)\n    return data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dependent_column(data,column):\n    \"\"\"\n    Removes all the Next Day columns.\n    Removes all the non Growth Rate Columns (GR)\n    add the predictor column to list of columns.\n\n    Parameters\n    ----------\n    data : dataframe\n\n    column : string\n        name of the predictor column \n\n    Returns\n    -------\n    data : dataframe\n        an updated dataframe after performing all the opertaions.\n    column : string\n        name of the predictor column\n    \"\"\"\n    cols = [col for col in data.columns if \"next\" not in col.lower() and col.lower().endswith(\"gr\")]\n    cols.append(column)\n    data = data[cols]\n    return (data,column)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reshape_data(x_train, x_test, y_train, y_test):\n    x_train = np.array(x_train)\n    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1],1))\n    x_test = np.array(x_test)\n    x_test = np.reshape(x_test, (x_test.shape[0],x_test.shape[1],1))\n    y_test = np.array(y_test)\n    y_test = np.reshape(y_test, (y_test.shape[0],1))\n\n    return (x_train, x_test, y_train, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def error_metrics(y_true, y_pred):\n    rmse = sqrt(metrics.mean_squared_error(y_true, y_pred))\n    mae = metrics.mean_absolute_error(y_true, y_pred)\n    mse = metrics.mean_squared_error(y_true, y_pred)\n    return {\"root_mean_squared_error\":rmse,\"mean_absolute_error\":mae,\"mean_squared_error\":mse}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_confusion_matrix(y_true,y_pred):\n    cm = confusion_matrix(y_true,y_pred)\n    accuracy = metrics.accuracy_score(y_true,y_pred)\n    precision = metrics.precision_score(y_true,y_pred)\n    recall = metrics.recall_score(y_true,y_pred)\n    f1_score = metrics.f1_score(y_true,y_pred)\n    return {\"accuracy\":accuracy,\"precision\":precision,\"recall\":recall,\"f1_score\":f1_score,\"confusion matrix\":cm}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_model(input_shape,optimizer='adam',activation='relu',dropout_rate=0.2,neurons=32,layers=3):\n    model = Sequential()\n    model.add(Conv1D(neurons, 1, activation=activation, input_shape=input_shape))\n    model.add(Flatten())\n    for i in range(layers):\n        model.add(Dense(neurons, activation=activation))\n        model.add(Dropout(dropout_rate))\n    model.add(Dense(1,activation=activation))\n    model.compile(loss='binary_crossentropy',optimizer=optimizer,)\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_process_model(optimizer = 'SGD',activation='relu',dropout_rate=0.2,neurons=64,layers=3,batch_size=64,epochs=25):\n    print(optimizer,activation,dropout_rate,neurons,layers,batch_size,epochs)\n    try:\n        global x_train, x_test, y_train, y_test\n        input_shape = (x_train.shape[1],1)\n        model = create_model(input_shape,optimizer=optimizer,activation=activation,dropout_rate=float(dropout_rate),neurons=int(neurons),layers=int(layers))\n        history = model.fit(x_train,y_train,epochs = int(epochs) ,batch_size = int(batch_size), validation_data = (x_test,y_test),shuffle = False,verbose=0)\n        y_pred = model.predict(x_test)\n        s = 0.01\n        t = 0.5\n        while True:\n            pred = np.array(y_pred>t,int)\n            res = confusion_matrix(y_test,pred)\n            if res[1][1] >= 100 or t <= 0:\n              break\n            t = t - s\n        result = {}\n        result.update({\"threshold\":t})\n        \n        confusion = create_confusion_matrix(y_test,pred)\n        result.update(confusion)\n        result.update({'epochs':epochs,'batch_size':batch_size})    \n        result.update({'optimizer':optimizer,'activation':activation,'dropout_rate':dropout_rate,'neurons':neurons,'layers':layers})\n        return result\n    except Exception as e:\n        print(e)\n        return None","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# myresults = pd.DataFrame()\n# batch_size = [25,32,64,100]\n# epochs = [25,50,75,100]\n# optimizer = ['SGD','Adam']\n# # learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n# # momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n# activation = ['relu','sigmoid']\n# dropout_rate = [0,0.1, 0.2, 0.3]\n# neurons = [32,64,128]\n# layers = [2,3,4]\n# combos = np.array(np.meshgrid(optimizer,activation,dropout_rate, neurons,layers,batch_size,epochs)).T.reshape(-1,7)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute():    \n    best_epochs,best_batch_size,best_layer,best_neurons,best_dropout_rate,best_activation,best_optimizer = 25,25,2,32,0.2,'sigmoid','adam'\n    prev = 0\n    for b in [25,32,50,75,100]:\n        res = run_process_model(optimizer = 'adam',activation='sigmoid',dropout_rate=0.2,neurons=32,layers=2,batch_size=b,epochs=25)\n        if (res['precision'] > prev):\n            prev = res['precision']\n            best_batch_size = res['batch_size']\n        else:\n            break\n            \n    for e in [50,75,100]:\n        res = run_process_model(optimizer = 'adam',activation='sigmoid',dropout_rate=0.2,neurons=32,layers=2,batch_size=best_batch_size,epochs=e)\n        print(prev,res['precision'])\n        if (res['precision'] > prev):\n            prev = res['precision']\n            best_epochs = res['epochs']\n        else:\n            break\n            \n    for l in [3,4]:\n        res = run_process_model(optimizer = 'adam',activation='sigmoid',dropout_rate=0.2,neurons=32,layers=l,batch_size=best_batch_size,epochs=best_epochs)\n        print(prev,res['precision'])\n        if (res['precision'] > prev):\n            prev = res['precision']\n            best_layer = res['layers']\n        else:\n            break\n            \n    for n in [64]:\n        res = run_process_model(optimizer = 'adam',activation='sigmoid',dropout_rate=0.2,neurons=n,layers=best_layer,batch_size=best_batch_size,epochs=best_epochs)\n        print(prev,res['precision'])\n        if (res['precision'] > prev):\n            prev = res['precision']\n            best_neurons = res['neurons']\n        else:\n            break\n    \n    for d in [0.1,0]:\n        res = run_process_model(optimizer = 'adam',activation='sigmoid',dropout_rate=d,neurons=best_neurons,layers=best_layer,batch_size=best_batch_size,epochs=best_epochs)\n        print(prev,res['precision'])\n        if (res['precision'] > prev):\n            prev = res['precision']\n            best_dropout_rate = res['dropout_rate']\n        else:\n            break\n    \n    \n    for a in ['relu']:\n        res = run_process_model(optimizer = 'adam',activation=a,dropout_rate=best_dropout_rate,neurons=best_neurons,layers=best_layer,batch_size=best_batch_size,epochs=best_epochs)\n        print(prev,res['precision'])\n        if (res['precision'] > prev):\n            prev = res['precision']\n            best_activation = res['activation']\n        else:\n            break\n\n    for o in ['sgd']:\n        res = run_process_model(optimizer = o,activation=best_activation,dropout_rate=best_dropout_rate,neurons=best_neurons,layers=best_layer,batch_size=best_batch_size,epochs=best_epochs)\n        print(prev,res['precision'])\n        if (res['precision'] > prev):\n            prev = res['precision']\n            best_optimizer = res['optimizer']\n        else:\n            break\n    print(best_epochs,best_batch_size,best_layer,best_neurons,best_dropout_rate,best_activation,best_optimizer)\n    return (best_epochs,best_batch_size,best_layer,best_neurons,best_dropout_rate,best_activation,best_optimizer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"security_codes = list()\nfor filename in os.listdir(\"../input/newdata/grstocks\"):\n    security_codes.append(filename[2:-4])\nsecurity_codes.sort()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for name in security_codes:\n    try:\n        df = pd.read_csv(\"../input/newdata/grstocks/\"+\"gr\"+str(name)+\".csv\")\n        df = pre_process_data(df,60)\n        df,column = dependent_column(df,\"Next Day Close Price GR\")\n        myresult = list()\n        for t in [0.001,0.002,0.003,0.004,0.005]:\n            print(name,t)\n            try:\n                df[\"Target\"] = df[column].apply(lambda x : 1 if x >= t else 0)\n                X = df.drop(columns=[\"Target\",column])\n                Y = df[\"Target\"]\n                x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3,random_state = 0)\n                (x_train, x_test, y_train, y_test) = reshape_data(x_train, x_test, y_train, y_test)\n                input_shape = (x_train.shape[1],1)\n                best_epochs,best_batch_size,best_layer,best_neurons,best_dropout_rate,best_activation,best_optimizer = compute()\n                result = run_process_model(optimizer = best_optimizer,activation=best_activation,dropout_rate=best_dropout_rate,neurons=best_neurons,layers=best_layer,batch_size=best_batch_size,epochs=best_epochs)\n                result.update({\"rate_of_growth\":t})\n                myresult.append(result)\n            except:\n                continue\n        pd.DataFrame(myresult).to_csv(\"cnn_\"+str(name)+\".csv\",index=None)\n    except:\n        pass","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}