{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import metrics\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\nfrom keras.backend import sigmoid\nfrom sklearn.metrics import confusion_matrix\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.metrics import precision_score, make_scorer\nimport matplotlib.pyplot as plt\nfrom math import sqrt\nimport threading\nfrom multiprocessing.pool import ThreadPool\nimport time\nimport multiprocessing\nimport traceback\nimport tensorflow as tf\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings; warnings.simplefilter('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pre_process_data(data,null_threshold):\n    \"\"\"\n    Drops Date and Unix Date columns from the data.\n    Drops the columns which has null values more than specified null_threshold.\n    Replaces infinite values with NAN.\n    Drops the rows which has null values.\n\n    Parameters\n    ----------\n    data : dataframe\n\n    null_threshold : numeric\n        numeric value describing the amount of null values that can be present.\n\n    Returns\n    -------\n    data : dataframe\n        an updated dataframe after performing all the opertaions.\n    \"\"\"\n    \n    data.drop(columns=['Unix Date','Date'],axis=1,inplace=True)\n    total = data.shape[0]\n    for col in data.columns:\n        if null_threshold * total / 100 < data[col].isnull().sum():\n            data.drop(columns=[col],axis=1,inplace=True)\n    data.replace([np.inf, -np.inf], np.nan, inplace=True)\n    data = data.apply(pd.to_numeric,errors='coerce')\n    data.dropna(axis=0,inplace=True)\n    return data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dependent_column(data,column):\n    \"\"\"\n    Removes all the Next Day columns.\n    Removes all the non Growth Rate Columns (GR)\n    add the predictor column to list of columns.\n\n    Parameters\n    ----------\n    data : dataframe\n\n    column : string\n        name of the predictor column \n\n    Returns\n    -------\n    data : dataframe\n        an updated dataframe after performing all the opertaions.\n    column : string\n        name of the predictor column\n    \"\"\"\n    cols = [col for col in data.columns if \"next\" not in col.lower() and col.lower().endswith(\"gr\")]\n    cols.append(column)\n    data = data[cols]\n    return (data,column)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reshape_data(x_train, x_test, y_train, y_test,units = 30):\n    my_x_train = list()\n    my_y_train = list()\n    my_x_test = list()\n    my_y_test = list()\n    for i in range(x_train.shape[0]-units):\n        my_x_train.append(x_train.iloc[i:i+units,:])\n        my_y_train.append(y_train.iloc[i+units,])\n    \n    my_x_train = np.array(my_x_train)\n    my_x_train = np.reshape(my_x_train,(my_x_train.shape[0],my_x_train.shape[1],my_x_train.shape[2]))\n    \n    my_y_train = np.array(my_y_train)\n    my_y_train = np.reshape(my_y_train,(my_y_train.shape[0],1))\n    \n    for i in range(x_test.shape[0]-units):\n        my_x_test.append(x_test.iloc[i:i+units,:])\n        my_y_test.append(y_test.iloc[i+units,])\n        \n    my_x_test = np.array(my_x_test)\n    my_x_test = np.reshape(my_x_test,(my_x_test.shape[0],my_x_test.shape[1],my_x_test.shape[2]))\n    \n    my_y_test = np.array(my_y_test)\n    my_y_test = np.reshape(my_y_test,(my_y_test.shape[0],1))\n    \n    return (my_x_train, my_x_test, my_y_train, my_y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def error_metrics(y_true, y_pred):\n    rmse = sqrt(metrics.mean_squared_error(y_true, y_pred))\n    mae = metrics.mean_absolute_error(y_true, y_pred)\n    mse = metrics.mean_squared_error(y_true, y_pred)\n    return {\"root_mean_squared_error\":rmse,\"mean_absolute_error\":mae,\"mean_squared_error\":mse}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_confusion_matrix(y_true,y_pred):\n    \n    cm = confusion_matrix(y_true,y_pred)\n    accuracy = metrics.accuracy_score(y_true,y_pred)\n    precision = metrics.precision_score(y_true,y_pred)\n    recall = metrics.recall_score(y_true,y_pred)\n    f1_score = metrics.f1_score(y_true,y_pred)\n    return {\"accuracy\":accuracy,\"precision\":precision,\"recall\":recall,\"f1_score\":f1_score,\"confusion matrix\":cm}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_lstm(input_shape,optimizer,activation,dropout_rate,neurons,layers):\n    model = Sequential()\n    model.add(LSTM(neurons,return_sequences=True,input_shape=input_shape))\n    model.add(Dropout(dropout_rate))\n    model.add(LSTM(neurons))\n    model.add(Dropout(dropout_rate))\n    for _ in range(layers):\n        model.add(Dense(neurons))\n        model.add(Dropout(dropout_rate))\n    model.add(Dense(units=1,activation = activation))\n    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=[tf.metrics.Precision()])\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_dataset(X,Y,t):\n    tr = int(len(X)*t)\n    tt = len(X) - tr\n    xtr = X[:tr]\n    xtt = X[tr:tr+tt]\n    ytr = Y[:tr]\n    ytt = Y[tr:tr+tt]\n    return (xtr,xtt,ytr,ytt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_process_model(optimizer,activation,dropout_rate,neurons,layers,batch_size,epochs):\n    try:\n        print(optimizer,activation,dropout_rate,neurons,layers,batch_size,epochs)\n        global x_train, x_test, y_train, y_test\n        input_shape = (x_train.shape[1],x_train.shape[2])\n        model =  KerasClassifier(build_lstm,input_shape=input_shape,optimizer=optimizer,activation=activation,dropout_rate=dropout_rate,neurons=neurons,layers=layers)\n        history = model.fit(x_train,y_train,epochs = int(epochs) ,batch_size = int(batch_size), validation_data = (x_test,y_test),shuffle = False,verbose=0)\n        y_pred = model.predict(x_test)\n        result = {}\n        confusion = create_confusion_matrix(y_test,y_pred)\n        result.update(confusion)\n        result.update({'epochs':epochs,'batch_size':batch_size})\n        result.update({'optimizer':optimizer,'activation':activation,'dropout_rate':dropout_rate,'neurons':neurons,'layers':layers})\n        return result\n    except:\n        traceback.print_exc()\n        return []","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute():    \n    best_epochs,best_batch_size,best_layer,best_neurons,best_dropout_rate,best_activation,best_optimizer = 25,25,2,32,0.2,'sigmoid','adam'\n    prev = 0\n    for b in [25,32,50,75,100]:\n        res = run_process_model(optimizer = 'adam',activation='sigmoid',dropout_rate=0.2,neurons=32,layers=2,batch_size=b,epochs=25)\n        print(prev,res['precision'])\n        if (res['precision'] > prev):\n            prev = res['precision']\n            best_batch_size = res['batch_size']\n        else:\n            break\n            \n    for e in [50,75,100]:\n        res = run_process_model(optimizer = 'adam',activation='sigmoid',dropout_rate=0.2,neurons=32,layers=2,batch_size=best_batch_size,epochs=e)\n        print(prev,res['precision'])\n        if (res['precision'] > prev):\n            prev = res['precision']\n            best_epochs = res['epochs']\n        else:\n            break\n            \n    for l in [3,4]:\n        res = run_process_model(optimizer = 'adam',activation='sigmoid',dropout_rate=0.2,neurons=32,layers=l,batch_size=best_batch_size,epochs=best_epochs)\n        print(prev,res['precision'])\n        if (res['precision'] > prev):\n            prev = res['precision']\n            best_layer = res['layers']\n        else:\n            break\n\n    for n in [64]:\n        res = run_process_model(optimizer = 'adam',activation='sigmoid',dropout_rate=0.2,neurons=n,layers=best_layer,batch_size=best_batch_size,epochs=best_epochs)\n        print(prev,res['precision'])\n        if (res['precision'] > prev):\n            prev = res['precision']\n            best_neurons = res['neurons']\n        else:\n            break\n    \n    for d in [0.1,0]:\n        res = run_process_model(optimizer = 'adam',activation='sigmoid',dropout_rate=d,neurons=best_neurons,layers=best_layer,batch_size=best_batch_size,epochs=best_epochs)\n        print(prev,res['precision'])\n        if (res['precision'] > prev):\n            prev = res['precision']\n            best_dropout_rate = res['dropout_rate']\n        else:\n            break\n            \n    best_activation = 'sigmoid'\n    \n    for o in ['sgd']:\n        res = run_process_model(optimizer = o,activation=best_activation,dropout_rate=best_dropout_rate,neurons=best_neurons,layers=best_layer,batch_size=best_batch_size,epochs=best_epochs)\n        print(prev,res['precision'])\n        if (res['precision'] > prev):\n            prev = res['precision']\n            best_optimizer = res['optimizer']\n        else:\n            break\n    print(best_epochs,best_batch_size,best_layer,best_neurons,best_dropout_rate,best_activation,best_optimizer)\n    return (best_epochs,best_batch_size,best_layer,best_neurons,best_dropout_rate,best_activation,best_optimizer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"security_codes = list()\nfor filename in os.listdir(\"../input/newdata/grstocks\"):\n    security_codes.append(filename[2:-4])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for _,row in top.iterrows():\nfor name in security_codes:\n    try:\n        df = pd.read_csv(\"../input/newdata/grstocks/\"+\"gr\"+str(name)+\".csv\")\n        df = df.iloc[::-1].reset_index(drop=True)\n        df = pre_process_data(df,60)\n        df,column = dependent_column(df,\"Next Day Close Price GR\")\n        myresult = list()\n        for t in [0.001,0.002,0.003,0.004,0.005]:\n            print(name,t)\n            try:\n                df[\"Target\"] = df[column].apply(lambda x : 1 if x >= t else 0)\n                X = df.drop(columns=[\"Target\",column])\n                Y = df[\"Target\"]\n                x_train, x_test, y_train, y_test = split_dataset(X, Y,0.70)\n                (x_train, x_test, y_train, y_test) = reshape_data(x_train, x_test, y_train, y_test)\n                best_epochs,best_batch_size,best_layer,best_neurons,best_dropout_rate,best_activation,best_optimizer = compute()\n                result = run_process_model(optimizer = best_optimizer,activation=best_activation,dropout_rate=best_dropout_rate,neurons=best_neurons,layers=best_layer,batch_size=best_batch_size,epochs=best_epochs)\n                result.update({\"rate_of_growth\":t})\n                myresult.append(result)\n            except:\n                traceback.print_exc()\n        pd.DataFrame(myresult).to_csv(\"rnn_\"+str(name)+\".csv\",index=None)\n    except:\n        traceback.print_exc()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}